{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconstructed Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a deconstructed version of the demo that is intended to show a bit more detail about the operation of `prospector`. It runs over the same example data as in the `demo_params.py` parameter file, using a similar model.  However, in this demo the implicit methods of the parameter file are made explicit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set up some environmental dependencies. These just make the numerics easier and adjust some of the plotting defaults to make things more legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1069: UserWarning: Bad val \"$TEMPLATE_BACKEND\" on line #41\n",
      "\t\"backend      : $TEMPLATE_BACKEND\n",
      "\"\n",
      "\tin file \"/Applications/anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle\"\n",
      "\tKey backend: Unrecognized backend string \"$template_backend\": valid strings are [u'pgf', u'cairo', u'MacOSX', u'CocoaAgg', u'gdk', u'ps', u'GTKAgg', u'nbAgg', u'GTK', u'Qt5Agg', u'template', u'emf', u'GTK3Cairo', u'GTK3Agg', u'WX', u'Qt4Agg', u'TkAgg', u'agg', u'svg', u'GTKCairo', u'WXAgg', u'WebAgg', u'pdf']\n",
      "  (val, error_details, msg))\n",
      "/Applications/anaconda/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time, sys, os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import *\n",
    "# re-defining plotting defaults\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import gridspec\n",
    "from prospect.utils.obsutils import fix_obs\n",
    "import fsps\n",
    "import sedpy\n",
    "import prospect\n",
    "from prospect.models import priors\n",
    "from prospect.models.templates import TemplateLibrary\n",
    "rcParams.update({'xtick.major.pad': '7.0'})\n",
    "rcParams.update({'xtick.major.size': '7.5'})\n",
    "rcParams.update({'xtick.major.width': '1.5'})\n",
    "rcParams.update({'xtick.minor.pad': '7.0'})\n",
    "rcParams.update({'xtick.minor.size': '3.5'})\n",
    "rcParams.update({'xtick.minor.width': '1.0'})\n",
    "rcParams.update({'ytick.major.pad': '7.0'})\n",
    "rcParams.update({'ytick.major.size': '7.5'})\n",
    "rcParams.update({'ytick.major.width': '1.5'})\n",
    "rcParams.update({'ytick.minor.pad': '7.0'})\n",
    "rcParams.update({'ytick.minor.size': '3.5'})\n",
    "rcParams.update({'ytick.minor.width': '1.0'})\n",
    "rcParams.update({'xtick.color': 'k'})\n",
    "rcParams.update({'ytick.color': 'k'})\n",
    "rcParams.update({'font.size': 30})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need several things to run a fit.  These include \n",
    "  1. An `obs` dictionary (with the data we intend to fit)\n",
    "  2. A stellar population synthesis object (to predict spectra from parameters)\n",
    "  3. A `model` object (to store and translate parameters and priors)\n",
    "  4. A likelihood function\n",
    "  \n",
    "It can also useful to collect the meta-parameters contolling how the fit is done in a ``run_params`` dictionary. We will do that as we go along.  We will also encapsulate each step of the setup in a series of `load_x()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `load_obs` function \n",
    "It can be helpful to encapsulate all the logic for reading from your catalog into a single `load_obs` method.  Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obs(snr=10, ldist=10.0, **extras):\n",
    "    galex = ['galex_FUV', 'galex_NUV']\n",
    "    spitzer = ['spitzer_irac_ch'+n for n in ['1','2','3','4']]\n",
    "    sdss = ['sdss_{0}0'.format(b) for b in ['u','g','r','i','z']]\n",
    "    filternames = galex + sdss + spitzer\n",
    "    \n",
    "    # Here you could, e.g. read from a catalog\n",
    "    M_AB = np.array([-11.93, -12.37, -13.37, -14.22, -14.61, -14.86, \n",
    "                     -14.94, -14.09, -13.62, -13.23, -12.78])\n",
    "    dm = 25 + 5.0 * np.log10(ldist)\n",
    "    mags = M_AB + dm\n",
    "\n",
    "    obs = {}\n",
    "    obs[\"filters\"] = sedpy.observate.load_filters(filternames)\n",
    "    obs[\"maggies\"] = 10**(-0.4*mags)\n",
    "    obs[\"maggies_unc\"] = (1./snr) * obs[\"maggies\"]\n",
    "    obs[\"phot_mask\"] = np.array(['spitzer' not in f.name for f in obs[\"filters\"]])\n",
    "    obs[\"phot_wave\"] = [f.wave_effective for f in obs[\"filters\"]]\n",
    "    obs[\"wavelength\"] = None  # this would be a vector of wavelengths in angstroms if we had \n",
    "    obs[\"spectrum\"] = None\n",
    "    obs['unc'] = None  #spectral uncertainties are given here\n",
    "    obs['mask'] = None\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will store some meta-parameters that control the input arguments to this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phot_mask', 'phot_wave', 'ndof', 'mask', 'spectrum', 'maggies_unc', 'filternames', 'unc', 'maggies', 'filters', 'wavelength', 'logify_spectrum']\n",
      "7\n",
      "['galex_FUV', 'galex_NUV', 'sdss_u0', 'sdss_g0', 'sdss_r0', 'sdss_i0', 'sdss_z0', 'spitzer_irac_ch1', 'spitzer_irac_ch2', 'spitzer_irac_ch3', 'spitzer_irac_ch4']\n"
     ]
    }
   ],
   "source": [
    "run_params = {}\n",
    "run_params[\"snr\"] = 10.0\n",
    "run_params[\"ldist\"] = 10.0\n",
    "obs=load_obs(**run_params)\n",
    "obs = fix_obs(obs)\n",
    "print(obs.keys())\n",
    "print obs['ndof']\n",
    "print obs['filternames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model object\n",
    "\n",
    "Now we need a set of model parameters, which will **define** the model we are tying to fit to the data.  The model object stores the parameters that are used by the SPS object to build a spectrum, as well as infomation about which parameters are to be varied during fitting, and priors on those parameters.  It efficiently converts between a vector of parameter values (the `theta` attribute) used by the MCMC samplers or optimizers and the dictionary of parameter names and values (the `params` attribute) that can be passed to the sps objects' `get_spectrum()` method.\n",
    "\n",
    "To create the model object we need a list or dictionary of model parameters and some infomation about them.  Each parameter must a have a name, a length (vector parameters are possible), an initial value, and must be specified as either a free parameter or a fixed parameter.  If it is a free parameter it needs a prior as well, which we will get from the `priors` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building by hand\n",
    "Let's start by creating a dictionary that describes a single parameter controlling the stellar mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's the description for one parameter.  Let's **build up the rest of our model** as a dictionary of these parameter descriptions.  At a minimum we will need some sort of distance or redshift information (which in this example is a fixed parameter), and something descibing the SFH.  We could also add parameters controlling metallicity, dust attenuation and emission, nebular emission, even the IMF. Note that any parameter whose value is not explicitly specified via a model parameter dictionary will be given the default value from python-FSPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to build up this model is to start with some predefined parameter sets from the `prospect.models.templates` module. First, lets look at what pre-packaged parameter sets are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `\"parametric_sfh\"` parameter set will do most of what we want.  Let's look at it in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n",
      "Free Parameters: (name: prior) \n",
      "-----------\n",
      "  dust1: <class 'prospect.models.priors.TopHat'>(mini=0.0,maxi=4.0)\n",
      "  duste_qpah: <class 'prospect.models.priors.TopHat'>(mini=0.5,maxi=7.0)\n",
      "  total_mass: <class 'prospect.models.priors.LogUniform'>(mini=100000000.0,maxi=1e+12)\n",
      "  dust_index: <class 'prospect.models.priors.TopHat'>(mini=-2.0,maxi=0.5)\n",
      "  duste_gamma: <class 'prospect.models.priors.LogUniform'>(mini=0.001,maxi=0.15)\n",
      "  agn_tau: <class 'prospect.models.priors.LogUniform'>(mini=5.0,maxi=150.0)\n",
      "  logzsol: <class 'prospect.models.priors.TopHat'>(mini=-2,maxi=0.19)\n",
      "  dust2: <class 'prospect.models.priors.TopHat'>(mini=0.0,maxi=4.0)\n",
      "  fagn: <class 'prospect.models.priors.LogUniform'>(mini=1e-05,maxi=3.0)\n",
      "  z_fraction: <class 'prospect.models.priors.Beta'>(mini=0.0,beta=[1 1 1 1 1],maxi=1.0,alpha=[5 4 3 2 1])\n",
      "  duste_umin: <class 'prospect.models.priors.TopHat'>(mini=0.1,maxi=25)\n",
      "\n",
      "Fixed Parameters: (name: value [, depends_on]) \n",
      "-----------\n",
      "  add_dust_emission: True \n",
      "  dust_type: 4 \n",
      "  gas_logu: -2.0 \n",
      "  add_neb_emission: True \n",
      "  sfh: 3 \n",
      "  add_agn_dust: True \n",
      "  gas_logz: 0.0 <function stellar_logzsol at 0x14d7bcd70>\n",
      "  nebemlineinspec: True \n",
      "  imf_type: 2 \n",
      "  agebins: [[ 0.          8.        ]\n",
      " [ 8.          8.47712125]\n",
      " [ 8.47712125  9.        ]\n",
      " [ 9.          9.47712125]\n",
      " [ 9.47712125  9.77815125]\n",
      " [ 9.77815125 10.13353891]] \n",
      "  zred: 0.1 \n",
      "  mass: 1.0 <function zfrac_to_masses at 0x14d7c6320>\n",
      "  add_neb_continuum: True \n"
     ]
    }
   ],
   "source": [
    "#TemplateLibrary.describe(\"parametric_sfh\")\n",
    "#TemplateLibrary.describe(\"agn\")\n",
    "\n",
    "print '###############################'\n",
    "TemplateLibrary.describe(\"alpha\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good.  We'll just adjust a couple of the initial values and priors, and add a parameter that will set the distance even though the redshift of theses objects is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Manipulating the `model` object\n",
    "Great.  We now have a model.  We're fitting for 5 parameters here:\n",
    "- stellar mass *formed* $M_\\star$,\n",
    "- metallicity $\\log Z/Z_\\odot$, \n",
    "- age $t$ of the galaxy \n",
    "- star formation timescale $\\tau$ for an exponentially declining star formation history (SFH), and\n",
    "- dust attenuation of old stellar populations $A_V$.\n",
    "\n",
    "Everything else here is fixed explicitly.  There are many other `sps` parameters that are set implicitly in the FSPS defaults.  Note that by default the stellar mass here refers to the stellar mass *formed* by the given age, which will always be slightly higher than the *surviving* stellar mass, due to mass loss during stellar evolution (winds, SNe, etc.)\n",
    "\n",
    "Let's change some of the parameter values in a couple different ways, and look at the prior probablity for those new parameter combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `load_model` function\n",
    "As for the ``obs`` dictionary and the ``sps`` object it can be sueful to define a ``load_model`` method that instantiates the ``model`` object.  This method can also be used to change the parameter specification dictionaries according to adjustable arguments.  Let's do an example where we change whether the metallicity is a free and fitted parameter or fixed to a particular value, optionally turn on dust emission, and where we can set the redshift by hand as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "['total_mass', 'dust_index', 'dust1', 'logzsol', 'dust2', 'duste_qpah', 'z_fraction', 'duste_umin', 'agn_tau', 'duste_gamma', 'fagn']\n"
     ]
    }
   ],
   "source": [
    "def load_model(object_redshift=None, fixed_metallicity=None, add_dust=False, \n",
    "               ldist=10.0, **extras):\n",
    "    \n",
    "    from prospect.models.sedmodel import SedModel\n",
    "    from prospect.models.templates import TemplateLibrary\n",
    "\n",
    "    model_params = TemplateLibrary[\"alpha\"]\n",
    "    \n",
    "    # Add lumdist parameter\n",
    "    model_params[\"lumdist\"] = {\"N\": 1, \"isfree\": False, \"init\": ldist, \"units\":\"Mpc\"}\n",
    "    \n",
    "    model_params[\"zred\"]['init'] = object_redshift\n",
    "    # adjust priors\n",
    "    model_params[\"duste_qpah\"][\"isfree\"] = True\n",
    "    model_params[\"duste_umin\"][\"isfree\"] = True\n",
    "    model_params[\"duste_gamma\"][\"isfree\"] = True\n",
    "    #model_params[\"dust1\"][\"isfree\"] = True\n",
    "    \n",
    "    model_params[\"zred\"][\"init\"] = 0.0\n",
    "    model_params[\"dust2\"][\"init\"] = 0.3\n",
    "    model_params[\"logzsol\"][\"init\"] = -0.5\n",
    "    model_params[\"total_mass\"][\"init\"] = 1e8\n",
    "    \n",
    "    #model_params['z_fraction']['prior'] = priors.Beta(alpha=1.0,beta=1.0,mini=0.0,maxi=1.0)\n",
    "    model_params[\"dust_index\"][\"prior\"] = priors.TopHat(mini=-2.2, maxi=0.4)\n",
    "    model_params[\"logzsol\"][\"prior\"] = priors.TopHat(mini=-2.0, maxi=0.2)\n",
    "    model_params[\"duste_qpah\"][\"prior\"] = priors.TopHat(mini=0.1, maxi=10)\n",
    "    model_params[\"duste_umin\"][\"prior\"] = priors.TopHat(mini=0.1, maxi=25)\n",
    "    model_params[\"duste_gamma\"][\"prior\"] = priors.TopHat(mini=1e-6, maxi=1.0)\n",
    "    model_params[\"dust2\"][\"prior\"] = priors.TopHat(mini=1e-6, maxi=3.0)\n",
    "    model_params[\"agn_tau\"][\"prior\"] = priors.LogUniform(mini=5, maxi=1.5e2)\n",
    "    model_params[\"total_mass\"][\"prior\"] = priors.LogUniform(mini=1e5, maxi=1e14)\n",
    "    # If we are going to be using emcee, it is useful to provide a \n",
    "    # minimum scale for the cloud of walkers (the default is 0.1)\n",
    "    model_params[\"total_mass\"][\"disp_floor\"] = 1e5\n",
    "    model_params[\"agn_tau\"][\"disp_floor\"] = 1\n",
    "    model_params[\"duste_gamma\"][\"disp_floor\"] = 0.1\n",
    "\n",
    "    \n",
    "    # Change the model parameter specifications based on some keyword arguments\n",
    "\n",
    "    if object_redshift is not None:\n",
    "        # make sure zred is fixed\n",
    "        model_params[\"zred\"]['isfree'] = False\n",
    "        # And set the value to the object_redshift keyword\n",
    "        model_params[\"zred\"]['init'] = object_redshift\n",
    "        \n",
    "\n",
    "        \n",
    "    # Now instantiate the model using this new dictionary of parameter specifications\n",
    "    model = SedModel(model_params)\n",
    "\n",
    "    return model\n",
    "\n",
    "run_params[\"object_redshift\"] = 0.0\n",
    "\n",
    "\n",
    "model = load_model(**run_params)\n",
    "#print model.fixed_params\n",
    "print '---------'\n",
    "print model.free_params\n",
    "\n",
    "#print model.init_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `sps` object\n",
    "We need an object that will build SEDs for a given set of parameters.  In `prospector` we call these **sps** objects.  Given a dictionary of parameters (provided by the model object), they must be able to return a spectrum, photometry, and maybe some ancillary information.  This is often done using large spectral libraries and, for galaxies, isochrone information.  Typically in `prospector` we use `fsps.StellarPopulation` objects, under thin wrappers that add a little functionality and change the API a bit. The different wrappers correspond to different SFH parameterizations. Here we use `CSPSpecBasis` which works with (linear combinations of) composite stellar populations as described in the FSPS manual with `sfh_type` of 1, 4, or 5.  Other `sps` objects can be used for non-parameteric SFH, notable `prospect.sources.FastStepBasis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `load_sps` function\n",
    "Again, it can be helpful to encapsulate the loading of the sps object in a `load_sps` method, with meta-parameters controlling the how the object is instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sps(zcontinuous=1, **extras):\n",
    "    from prospect.sources import FastStepBasis\n",
    "    sps = FastStepBasis(zcontinuous=zcontinuous)\n",
    "    return sps\n",
    "\n",
    "run_params[\"zcontinuous\"] = 1\n",
    "sps = load_sps(**run_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the model\n",
    "Now that we have the `sps` object we can also generate a prediction for the data from any set of model parameters.  To see how this works, lets make an SED!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that creating a new model with FSPS is somewhat time-intensive (of order seconds), but once the relevant SSPs have been built they are subsequently stored in cache so similar models can be generated much more quickly (of order milliseconds, unless you are changing parameters that affect the SSPs, like the IMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood function\n",
    "Now all we are missing is a likelihood function.  In most cases, this will simply be a function of the **spectral likelihood** and a **photometric likelihood** such that\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = f(\\mathcal{L}_{\\textrm{spec}}, \\mathcal{L}_{\\textrm{phot}}) \\quad .\n",
    "$$\n",
    "\n",
    "Assuming our errors are Normal (i.e. Gaussian), the log-likelihoods for each component are extremely straightforward to define and can be imported directly from Prospector.  How we choose to combine these likelihoods might vary depending on the particulars of our data. For the demo, our likelihood function for our model parameters $\\boldsymbol{\\theta}$ is just\n",
    "\n",
    "$$\n",
    "\\ln\\mathcal{L}(\\boldsymbol{\\theta}) = \\ln\\mathcal{L}_{\\textrm{spec}}(\\boldsymbol{\\theta}) + \\ln\\mathcal{L}_{\\textrm{phot}}(\\boldsymbol{\\theta}) \\quad .\n",
    "$$\n",
    "\n",
    "Below is a simple version of the likelihood function used in `prospector`.  Note that more complicated likelihoods including covariant noise and fitted noise parameters are possible, using special NoiseModel classes within `prospector`.  Also we are using the globally defined `obs`, `model`, and `sps`; in principle these could be passed as arguments to the `lnprobfn`.\n",
    "\n",
    "For nested sampling `lnprobfn(theta, nested=True)` will return the likelihood (since the prior probability is accounted for by drawing proposals from the priors), while for other types of MCMC sampling `lnprobfn(theta, nested=False)` returns the posterior probability.\n",
    "\n",
    "When using the provided scripts (`prospector.py` and `prospector_nest.py`) you don't have to specify the likelihood function yourself, it is defined in those scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.likelihood import lnlike_spec, lnlike_phot, write_log\n",
    "from prospect.models import priors\n",
    "verbose = False\n",
    "def lnprobfn(theta, nested=False, verbose=verbose):\n",
    "    \"\"\"\n",
    "    Given a parameter vector, a dictionary of observational data \n",
    "    a model object, and an sps object, return the ln of the posterior. \n",
    "    This requires that an sps object (and if using spectra \n",
    "    and gaussian processes, a GP object) be instantiated.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate prior probability and exit if not within prior\n",
    "    # Also if doing nested sampling, do not include the basic priors, \n",
    "    # since the drawing method includes the prior probability\n",
    "    lnp_prior = model.prior_product(theta, nested=nested)\n",
    "    if not np.isfinite(lnp_prior):\n",
    "        return -np.infty\n",
    "        \n",
    "    # Generate \"mean\" model\n",
    "    t1 = time.time()\n",
    "    spec, phot, mfrac = model.mean_model(theta, obs, sps=sps)\n",
    "    d1 = time.time() - t1\n",
    " \n",
    "    # Calculate likelihoods\n",
    "    t2 = time.time()\n",
    "    lnp_spec = lnlike_spec(spec, obs=obs)\n",
    "    lnp_phot = lnlike_phot(phot, obs=obs)\n",
    "    d2 = time.time() - t2\n",
    "    if verbose:\n",
    "        write_log(theta, lnp_prior, lnp_spec, lnp_phot, d1, d2)\n",
    "\n",
    "    return lnp_prior + lnp_phot + lnp_spec\n",
    "\n",
    "run_params[\"verbose\"] = verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be useful for some optimization methods (i.e. Levenberg-Marquardt) to define a function that returns the vector of chi-square residuals.  Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.likelihood import chi_spec, chi_phot\n",
    "def chivecfn(theta):\n",
    "    \"\"\"A version of lnprobfn that returns the simple uncertainty \n",
    "    normalized residual instead of the log-posterior, for use with \n",
    "    least-squares optimization methods like Levenburg-Marquardt.\n",
    "    \n",
    "    It's important to note that the returned chi vector does not \n",
    "    include the prior probability.\n",
    "    \"\"\"\n",
    "    lnp_prior = model.prior_product(theta)\n",
    "    if not np.isfinite(lnp_prior):\n",
    "        return -np.infty\n",
    "\n",
    "    # Generate mean model\n",
    "    t1 = time.time()\n",
    "    try:\n",
    "        spec, phot, x = model.mean_model(theta, obs, sps=sps)\n",
    "    except(ValueError):\n",
    "        return -np.infty\n",
    "    d1 = time.time() - t1\n",
    "\n",
    "    chispec = chi_spec(spec, obs)\n",
    "    chiphot = chi_phot(phot, obs)\n",
    "    return np.concatenate([chispec, chiphot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Prospector\n",
    "Now that we have defined the model and set up the data that we want to fit, we are ready to run prospector.  We will do this in a few steps.  First we will run all the convenience functions we made earlier to get the fitting ingredients and set up the output.  Then we will conduct a $\\chi^2$ minimization. Finally we will run an ensemble MCMC sampler around the best location form the minimization and save the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing\n",
    "Here we will run all our convenience functions and set up the output file(s) where our results will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Outputs\n",
    "We are just about ready to do the inference. But before we start running anything, let's just check whether we can write out our fits to an HDF5 file. If not, we'll just dump them as a pickle at the end.  As the HDF5 output is set up, the `model_params` list, the `obs` dictionary, and the `run_params` dictionary are serialized (if possible) and added as attributes or datasets in the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.io import write_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file demo_1560288828_mcmc.h5\n",
      "Could not serialize model_params\n"
     ]
    }
   ],
   "source": [
    "run_params[\"outfile\"] = 'demo'\n",
    "outroot = \"{0}_{1}\".format(run_params['outfile'], int(time.time()))\n",
    "try:\n",
    "    hfilename = outroot + '_mcmc.h5'\n",
    "    hfile = h5py.File(hfilename, \"a\")\n",
    "    print(\"Writing to file {}\".format(hfilename))\n",
    "    write_results.write_h5_header(hfile, run_params, model)\n",
    "    write_results.write_obs_to_h5(hfile, obs)\n",
    "except:\n",
    "    hfile = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization Step\n",
    "We can attempt to initialize our model reasonably close to the data by using some numerical minimization routines.\n",
    "Here we will use Levenberg-Marquardt. Keywords that control the optimization algorithm will again be stored in the `run_params` dictionary. Levenberg-Marquardt requires a likelihood function that returns a vector of chi values, not an actual likelihood, so we will write that. It's important to note that in the function below we do *not* account for prior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect import fitting\n",
    "from scipy.optimize import least_squares\n",
    "run_params[\"nmin\"] = 5\n",
    "run_params['ftol'] = 3e-16 \n",
    "run_params['maxfev'] = 5000\n",
    "run_params['xtol'] = 3e-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+08  0.00000000e+00  0.00000000e+00 -5.00000000e-01\n",
      "  3.00000000e-01  4.00000000e+00  8.33333333e-01  8.00000000e-01\n",
      "  7.50000000e-01  6.66666667e-01  5.00000000e-01  1.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.00000000e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/prospect-0.2-py2.7.egg/prospect/models/priors.py:155: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Residuals are not finite in the initial point.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cc66b9da7d5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     res = least_squares(chivecfn, np.array(pinit), method='lm', x_scale='jac',\n\u001b[1;32m     16\u001b[0m                         \u001b[0mxtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xtol\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ftol\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                         max_nfev=run_params[\"maxfev\"])\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mguesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/scipy/optimize/_lsq/least_squares.pyc\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Residuals are not finite in the initial point.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Residuals are not finite in the initial point."
     ]
    }
   ],
   "source": [
    "# --- start minimization ----\n",
    "# find the best initial value\n",
    "min_method = 'levenberg_marquardt'\n",
    "run_params[\"min_method\"] = min_method\n",
    "\n",
    "# We'll start minimization from \"nmin\" separate places, \n",
    "# the first based on the \"init\" values of each parameter and the \n",
    "# rest drawn from the prior.  This can guard against local minima.\n",
    "nmin = run_params[\"nmin\"]\n",
    "ts = time.time()  # time it\n",
    "pinitial = fitting.minimizer_ball(model.initial_theta.copy(), nmin, model)\n",
    "print model.initial_theta.copy() \n",
    "guesses = []\n",
    "for i, pinit in enumerate(pinitial): #loop over initial guesses\n",
    "    res = least_squares(chivecfn, np.array(pinit), method='lm', x_scale='jac',\n",
    "                        xtol=run_params[\"xtol\"], ftol=run_params[\"ftol\"], \n",
    "                        max_nfev=run_params[\"maxfev\"])\n",
    "    guesses.append(res)\n",
    "\n",
    "# Calculate chi-square of the results, and choose the best one\n",
    "# fitting.reinitialize moves the parameter vector away from edges of the prior.\n",
    "chisq = [np.sum(r.fun**2) for r in guesses]\n",
    "best = np.argmin(chisq)\n",
    "theta_best = fitting.reinitialize(guesses[best].x, model,\n",
    "                                  edge_trunc=run_params.get('edge_trunc', 0.1))\n",
    "initial_prob = None\n",
    "pdur = time.time() - ts\n",
    "\n",
    "# output results\n",
    "print('done {0} in {1}s'.format(min_method, pdur))\n",
    "print('best {0} chi-sq: {1}'.format(min_method, chisq[best]))\n",
    "print('best guess paramaters:')\n",
    "for k, t in zip(model.theta_labels(), theta_best):\n",
    "    print('  {} = {}'.format(k, t))\n",
    "print pdur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model after minimization\n",
    "Now let's see how our model looks in the data space after minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should look much better, except maybe for the filters that we have masked out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Posterior\n",
    "Now that we're somewhat burned in, we can begin sampling from the posterior using **Markov Chain Monte Carlo** (MCMC). Prospector by default uses **emcee**, and will try to parallelize the process over multiple cores when available through MPI and mpi4py. In this interactive notebook though we will assume single-threaded operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `emcee` algorithm requires several options to be specified, related to the number of walkers, the number of iterations, and to rounds of burn-in.  For convenience we will store these in the `run_params` meta-parameter dictionary that we've been using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of emcee walkers\n",
    "run_params[\"nwalkers\"] = 64\n",
    "# Number of iterations of the MCMC sampling\n",
    "run_params[\"niter\"] = 512\n",
    "# Number of iterations in each round of burn-in\n",
    "# After each round, the walkers are reinitialized based on the \n",
    "# locations of the highest probablity half of the walkers.\n",
    "run_params[\"nburn\"] = [16, 32, 64]\n",
    "# The following number controls how often the chain is written to disk. This can be useful \n",
    "# to make sure that not all is lost if the code dies during a long MCMC run. It ranges \n",
    "# from 0 to 1; the current chains will be written out every `interval` * `niter` iterations.\n",
    "# The default is 1, i.e. only write out at the end of the run.\n",
    "run_params[\"interval\"] = 0.25 # write out after every 25% of the sampling is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and start sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress output\n",
    "fout = sys.stdout\n",
    "fnull = open(os.devnull, 'w')\n",
    "sys.stdout = fnull\n",
    "\n",
    "# set the initial center of the ball of walkers to the best optimization result\n",
    "initial_center = theta_best.copy()\n",
    "\n",
    "# Start sampling\n",
    "tstart = time.time()  # time it\n",
    "out = fitting.run_emcee_sampler(lnprobfn, initial_center, model,\n",
    "                                pool=None, hdf5=hfile, **run_params)\n",
    "esampler, burn_loc0, burn_prob0 = out\n",
    "edur = time.time() - tstart\n",
    "\n",
    "sys.stdout = fout\n",
    "\n",
    "print('done emcee in {0}s'.format(edur))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Output\n",
    "Now that everything's all set, let's save our results to disk.  These will be written to 2 or 3 files beginning with the value of `outroot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results.write_hdf5(hfile, run_params, model, obs, \n",
    "                         esampler, guesses,\n",
    "                         toptimize=pdur, tsample=edur,\n",
    "                         sampling_initial_center=initial_center,\n",
    "                         post_burnin_center=burn_loc0,\n",
    "                         post_burnin_prob=burn_prob0)\n",
    "\n",
    "# The code below can be used to write python pickles (like IDL save files) to disk.  \n",
    "# These are not necessary, but can be convenient.\n",
    "#write_results.write_pickles(run_params, model, obs, esampler, guesses,\n",
    "#                            outroot=outroot, toptimize=pdur, tsample=edur,\n",
    "#                            sampling_initial_center=initial_center,\n",
    "#                            post_burnin_center=burn_loc0,\n",
    "#                            post_burnin_prob=burn_prob0)\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print edur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few basic plotting tools available to do a quick check on the results available in *prospect.io.read_results* and *prospect.utils.plotting*. We'll hack a few of these together in *plot_utils* here in the demo folder to make them a bit more amenable to plotting in this notebook.  We'll also import some functions useful for reading the output files we made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.io.read_results import results_from, get_sps\n",
    "from prospect.io.read_results import traceplot, subcorner\n",
    "print(outroot) # This is the start of the filename where we saved the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading output files\n",
    "Reading our results from our Pickle or HDF5 file is straightforward using the `results_from` method.  This returns a \"results\" dictionary, the `obs` dictionary of data to which the model was fit, and the `SedModel` object that was used in the fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab results (dictionary), the obs dictionary, and our corresponding models\n",
    "# When using parameter files set `dangerous=True`\n",
    "res, obs, mod = results_from(\"{}_mcmc.h5\".format(outroot), dangerous=False)\n",
    "# let's look at what's stored in the `res` dictionary\n",
    "print(res.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting parameter traces\n",
    "To see how our MCMC samples look, we can examine a few traces (that is, the evolution of the parameter value with iteration in the MCMC chain.)  You can use these plots (and the chains more generally) to assess whether the MCMC has converged, or if you need to sample for more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = np.random.choice(run_params[\"nwalkers\"], \n",
    "                          size=10, replace=False)\n",
    "tracefig = traceplot(res, figsize=(20,10), chains=chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a corner plot\n",
    "Our samples more generally can be shown using a corner/triangle plot.  The `subtriangle()` method below is a very thin wrapper on Dan Foreman-Mackey's **corner.py** code.  We'll overplot the MAP value as blue lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum a posteriori (of the locations visited by the MCMC sampler)\n",
    "imax = np.argmax(res['lnprobability'])\n",
    "i, j = np.unravel_index(imax, res['lnprobability'].shape)\n",
    "theta_max = res['chain'][i, j, :].copy()\n",
    "\n",
    "print('Optimization value: {}'.format(initial_center))\n",
    "print('MAP value: {}'.format(theta_max))\n",
    "cornerfig = subcorner(res, start=0, thin=5, truths=theta_max, \n",
    "                      fig=subplots(5,5,figsize=(27,27))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at SEDs and residuals\n",
    "Finally, let's just take a look at a random model drawn from our chains, and at the highest posterior probability model in the chain.  In this notebook we already have the `sps` object instantiated, but in general we may have to regenerate it based on information stored in the output file using the `prospect.io.read_results.get_sps` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly chosen parameters from chain\n",
    "randint = np.random.randint\n",
    "nwalkers, niter = run_params['nwalkers'], run_params['niter']\n",
    "theta = res['chain'][randint(nwalkers), randint(niter)]\n",
    "\n",
    "# generate models\n",
    "# sps = get_sps(res)  # this works if using parameter files\n",
    "mspec, mphot, mextra = model.mean_model(theta, obs, sps=sps)\n",
    "mspec_map, mphot_map, _ = model.mean_model(theta_max, obs, sps=sps)\n",
    "\n",
    "\n",
    "#plot setting-----\n",
    "theta = model.theta.copy()\n",
    "theta[model.theta_index[\"mass\"]] = 1e8\n",
    "initial_spec, initial_phot, initial_mfrac = model.sed(theta, obs=obs, sps=sps)\n",
    "# spec, phot, x = sps.get_spectrum(outwave=obs['wavelength'], filters=obs[\"filters\"], **model.params)\n",
    "title_text = ','.join([\"{}={}\".format(p, model.params[p][0]) for p in model.free_params])\n",
    "\n",
    "a = 1.0 + model.params.get('zred', 0.0) # cosmological redshifting\n",
    "# photometric effective wavelengths\n",
    "wphot = obs[\"phot_wave\"]\n",
    "# spectroscopic wavelengths\n",
    "if obs[\"wavelength\"] is None:\n",
    "    # *restframe* spectral wavelengths, since obs[\"wavelength\"] is None\n",
    "    wspec = sps.wavelengths\n",
    "    wspec *= a #redshift them\n",
    "else:\n",
    "    wspec = obs[\"wavelength\"]\n",
    "    \n",
    "xmin, xmax = np.min(wphot)*0.8, np.max(wphot)/0.8\n",
    "temp = np.interp(np.linspace(xmin,xmax,10000), wspec, initial_spec)\n",
    "ymin, ymax = temp.min()*0.8, temp.max()/0.4\n",
    "\n",
    "\n",
    "# Make plot of data and model\n",
    "figure(figsize=(16,8))\n",
    "\n",
    "loglog(wspec, mspec, label='Model spectrum (random draw)',\n",
    "       lw=0.7, color='navy', alpha=0.7)\n",
    "loglog(wspec, mspec_map, label='Model spectrum (MAP)',\n",
    "       lw=0.7, color='green', alpha=0.7)\n",
    "errorbar(wphot, mphot, label='Model photometry (random draw)',\n",
    "         marker='s', markersize=10, alpha=0.8, ls='', lw=3, \n",
    "         markerfacecolor='none', markeredgecolor='blue', \n",
    "         markeredgewidth=3)\n",
    "errorbar(wphot, mphot_map, label='Model photometry (MAP)',\n",
    "         marker='s', markersize=10, alpha=0.8, ls='', lw=3, \n",
    "         markerfacecolor='none', markeredgecolor='green', \n",
    "         markeredgewidth=3)\n",
    "errorbar(wphot, obs['maggies'], yerr=obs['maggies_unc'], \n",
    "         label='Observed photometry', ecolor='red', \n",
    "         marker='o', markersize=10, ls='', lw=3, alpha=0.8, \n",
    "         markerfacecolor='none', markeredgecolor='red', \n",
    "         markeredgewidth=3)\n",
    "\n",
    "# plot transmission curves\n",
    "for f in obs['filters']:\n",
    "    w, t = f.wavelength.copy(), f.transmission.copy()\n",
    "    while t.max() > 1:\n",
    "        t /= 10.\n",
    "    t = 0.1*(ymax-ymin)*t + ymin\n",
    "    loglog(w, t, lw=3, color='gray', alpha=0.7)\n",
    "\n",
    "xlabel('Wavelength [A]')\n",
    "ylabel('Flux Density [maggies]')\n",
    "xlim([xmin, xmax])\n",
    "ylim([ymin, ymax])\n",
    "legend(loc='best', fontsize=20)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  },
  "toc": {
   "nav_menu": {
    "height": "413px",
    "width": "290px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
